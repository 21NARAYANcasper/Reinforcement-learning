{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0SCPxrvGC3c"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy\n",
        "import gym\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the MountainCar-v0 environment\n",
        "env = gym.make('MountainCar-v0')  # Specify render_mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epL3y12fHuC-",
        "outputId": "c0df3afe-5890-4429-93d9-8291d828e88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the environment to get the initial state\n",
        "state = env.reset()\n",
        "\n",
        "# Set the maximum number of steps for the episode\n",
        "max_steps = 200"
      ],
      "metadata": {
        "id": "uCWnmgDaH2JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the episode\n",
        "for step in range(max_steps):\n",
        "    # Render the environment\n",
        "    env.render()\n",
        "\n",
        "    # Take a random action\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # Perform the action in the environment\n",
        "    step_info = env.step(action)\n",
        "    next_state, reward, done, _ = step_info[:4]  # Unpack the first four values\n",
        "\n",
        "    # Print information about the current state and action\n",
        "    print(f\"Step: {step}, Action: {action}, Next State: {next_state}, Reward: {reward}, Done: {done}\")\n",
        "\n",
        "    # Check if the episode is done\n",
        "    if done:\n",
        "        print(\"Episode finished after {} steps\".format(step + 1))\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXmXcrjJH4x1",
        "outputId": "73834309-747f-4e2a-9f17-045f618fd26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0, Action: 2, Next State: [-0.48813948  0.00074008], Reward: -1.0, Done: False\n",
            "Step: 1, Action: 2, Next State: [-0.48666483  0.00147463], Reward: -1.0, Done: False\n",
            "Step: 2, Action: 2, Next State: [-0.48446664  0.00219819], Reward: -1.0, Done: False\n",
            "Step: 3, Action: 0, Next State: [-0.48356128  0.00090538], Reward: -1.0, Done: False\n",
            "Step: 4, Action: 2, Next State: [-0.48195547  0.00160582], Reward: -1.0, Done: False\n",
            "Step: 5, Action: 1, Next State: [-0.48066115  0.0012943 ], Reward: -1.0, Done: False\n",
            "Step: 6, Action: 0, Next State: [-4.8068798e-01 -2.6837582e-05], Reward: -1.0, Done: False\n",
            "Step: 7, Action: 2, Next State: [-0.48003578  0.00065222], Reward: -1.0, Done: False\n",
            "Step: 8, Action: 1, Next State: [-4.7970933e-01  3.2642635e-04], Reward: -1.0, Done: False\n",
            "Step: 9, Action: 2, Next State: [-0.47871113  0.00099821], Reward: -1.0, Done: False\n",
            "Step: 10, Action: 2, Next State: [-0.47704858  0.00166257], Reward: -1.0, Done: False\n",
            "Step: 11, Action: 2, Next State: [-0.474734    0.00231457], Reward: -1.0, Done: False\n",
            "Step: 12, Action: 1, Next State: [-0.4727846  0.0019494], Reward: -1.0, Done: False\n",
            "Step: 13, Action: 1, Next State: [-0.47121483  0.00156977], Reward: -1.0, Done: False\n",
            "Step: 14, Action: 2, Next State: [-0.46903634  0.0021785 ], Reward: -1.0, Done: False\n",
            "Step: 15, Action: 2, Next State: [-0.46626523  0.00277111], Reward: -1.0, Done: False\n",
            "Step: 16, Action: 2, Next State: [-0.462922    0.00334322], Reward: -1.0, Done: False\n",
            "Step: 17, Action: 1, Next State: [-0.46003133  0.00289066], Reward: -1.0, Done: False\n",
            "Step: 18, Action: 2, Next State: [-0.45661455  0.00341679], Reward: -1.0, Done: False\n",
            "Step: 19, Action: 1, Next State: [-0.4536968   0.00291778], Reward: -1.0, Done: False\n",
            "Step: 20, Action: 1, Next State: [-0.45129943  0.00239735], Reward: -1.0, Done: False\n",
            "Step: 21, Action: 2, Next State: [-0.4484401   0.00285934], Reward: -1.0, Done: False\n",
            "Step: 22, Action: 1, Next State: [-0.44613966  0.00230042], Reward: -1.0, Done: False\n",
            "Step: 23, Action: 0, Next State: [-0.445415    0.00072469], Reward: -1.0, Done: False\n",
            "Step: 24, Action: 1, Next State: [-4.452713e-01  1.436722e-04], Reward: -1.0, Done: False\n",
            "Step: 25, Action: 1, Next State: [-4.4570971e-01 -4.3839245e-04], Reward: -1.0, Done: False\n",
            "Step: 26, Action: 1, Next State: [-0.44672698 -0.00101726], Reward: -1.0, Done: False\n",
            "Step: 27, Action: 0, Next State: [-0.44931567 -0.0025887 ], Reward: -1.0, Done: False\n",
            "Step: 28, Action: 2, Next State: [-0.45145687 -0.00214122], Reward: -1.0, Done: False\n",
            "Step: 29, Action: 2, Next State: [-0.45313495 -0.00167807], Reward: -1.0, Done: False\n",
            "Step: 30, Action: 0, Next State: [-0.4563376  -0.00320263], Reward: -1.0, Done: False\n",
            "Step: 31, Action: 1, Next State: [-0.46004125 -0.00370367], Reward: -1.0, Done: False\n",
            "Step: 32, Action: 1, Next State: [-0.46421874 -0.00417747], Reward: -1.0, Done: False\n",
            "Step: 33, Action: 2, Next State: [-0.46783918 -0.00362047], Reward: -1.0, Done: False\n",
            "Step: 34, Action: 2, Next State: [-0.4708759  -0.00303671], Reward: -1.0, Done: False\n",
            "Step: 35, Action: 1, Next State: [-0.4743064  -0.00343049], Reward: -1.0, Done: False\n",
            "Step: 36, Action: 2, Next State: [-0.47710523 -0.00279884], Reward: -1.0, Done: False\n",
            "Step: 37, Action: 2, Next State: [-0.47925162 -0.00214641], Reward: -1.0, Done: False\n",
            "Step: 38, Action: 2, Next State: [-0.48072967 -0.00147803], Reward: -1.0, Done: False\n",
            "Step: 39, Action: 1, Next State: [-0.48252833 -0.00179866], Reward: -1.0, Done: False\n",
            "Step: 40, Action: 2, Next State: [-0.48363423 -0.00110591], Reward: -1.0, Done: False\n",
            "Step: 41, Action: 2, Next State: [-4.8403919e-01 -4.0493035e-04], Reward: -1.0, Done: False\n",
            "Step: 42, Action: 2, Next State: [-4.8374012e-01  2.9906866e-04], Reward: -1.0, Done: False\n",
            "Step: 43, Action: 2, Next State: [-0.48273927  0.00100084], Reward: -1.0, Done: False\n",
            "Step: 44, Action: 0, Next State: [-4.8304412e-01 -3.0483890e-04], Reward: -1.0, Done: False\n",
            "Step: 45, Action: 2, Next State: [-4.8265237e-01  3.9175089e-04], Reward: -1.0, Done: False\n",
            "Step: 46, Action: 1, Next State: [-4.8256692e-01  8.5424501e-05], Reward: -1.0, Done: False\n",
            "Step: 47, Action: 2, Next State: [-0.48178846  0.00077846], Reward: -1.0, Done: False\n",
            "Step: 48, Action: 1, Next State: [-4.8132277e-01  4.6570657e-04], Reward: -1.0, Done: False\n",
            "Step: 49, Action: 2, Next State: [-0.4801733   0.00114949], Reward: -1.0, Done: False\n",
            "Step: 50, Action: 1, Next State: [-0.47934857  0.00082472], Reward: -1.0, Done: False\n",
            "Step: 51, Action: 0, Next State: [-0.47985476 -0.00050619], Reward: -1.0, Done: False\n",
            "Step: 52, Action: 1, Next State: [-0.48068807 -0.00083333], Reward: -1.0, Done: False\n",
            "Step: 53, Action: 1, Next State: [-0.48184234 -0.00115427], Reward: -1.0, Done: False\n",
            "Step: 54, Action: 0, Next State: [-0.48430896 -0.00246662], Reward: -1.0, Done: False\n",
            "Step: 55, Action: 0, Next State: [-0.4880696  -0.00376062], Reward: -1.0, Done: False\n",
            "Step: 56, Action: 1, Next State: [-0.49209616 -0.00402658], Reward: -1.0, Done: False\n",
            "Step: 57, Action: 2, Next State: [-0.49535868 -0.0032625 ], Reward: -1.0, Done: False\n",
            "Step: 58, Action: 1, Next State: [-0.4988327  -0.00347405], Reward: -1.0, Done: False\n",
            "Step: 59, Action: 0, Next State: [-0.50349236 -0.00465962], Reward: -1.0, Done: False\n",
            "Step: 60, Action: 1, Next State: [-0.5083026  -0.00481033], Reward: -1.0, Done: False\n",
            "Step: 61, Action: 2, Next State: [-0.51222765 -0.00392501], Reward: -1.0, Done: False\n",
            "Step: 62, Action: 1, Next State: [-0.5162379  -0.00401027], Reward: -1.0, Done: False\n",
            "Step: 63, Action: 1, Next State: [-0.5203034  -0.00406548], Reward: -1.0, Done: False\n",
            "Step: 64, Action: 2, Next State: [-0.52339363 -0.00309019], Reward: -1.0, Done: False\n",
            "Step: 65, Action: 0, Next State: [-0.5274853  -0.00409173], Reward: -1.0, Done: False\n",
            "Step: 66, Action: 1, Next State: [-0.5315479  -0.00406258], Reward: -1.0, Done: False\n",
            "Step: 67, Action: 1, Next State: [-0.5355509  -0.00400297], Reward: -1.0, Done: False\n",
            "Step: 68, Action: 2, Next State: [-0.53846425 -0.00291335], Reward: -1.0, Done: False\n",
            "Step: 69, Action: 1, Next State: [-0.54126614 -0.00280189], Reward: -1.0, Done: False\n",
            "Step: 70, Action: 0, Next State: [-0.5449356  -0.00366945], Reward: -1.0, Done: False\n",
            "Step: 71, Action: 2, Next State: [-0.5474451  -0.00250953], Reward: -1.0, Done: False\n",
            "Step: 72, Action: 1, Next State: [-0.54977596 -0.00233084], Reward: -1.0, Done: False\n",
            "Step: 73, Action: 0, Next State: [-0.5529107  -0.00313471], Reward: -1.0, Done: False\n",
            "Step: 74, Action: 1, Next State: [-0.5558258  -0.00291516], Reward: -1.0, Done: False\n",
            "Step: 75, Action: 0, Next State: [-0.5594996  -0.00367383], Reward: -1.0, Done: False\n",
            "Step: 76, Action: 1, Next State: [-0.5629047  -0.00340509], Reward: -1.0, Done: False\n",
            "Step: 77, Action: 2, Next State: [-0.56501573 -0.00211098], Reward: -1.0, Done: False\n",
            "Step: 78, Action: 1, Next State: [-0.56681687 -0.00180115], Reward: -1.0, Done: False\n",
            "Step: 79, Action: 0, Next State: [-0.5692948  -0.00247792], Reward: -1.0, Done: False\n",
            "Step: 80, Action: 2, Next State: [-0.57043105 -0.00113628], Reward: -1.0, Done: False\n",
            "Step: 81, Action: 0, Next State: [-0.5722173  -0.00178619], Reward: -1.0, Done: False\n",
            "Step: 82, Action: 2, Next State: [-5.7264012e-01 -4.2284114e-04], Reward: -1.0, Done: False\n",
            "Step: 83, Action: 2, Next State: [-0.57169646  0.00094364], Reward: -1.0, Done: False\n",
            "Step: 84, Action: 2, Next State: [-0.56939334  0.00230313], Reward: -1.0, Done: False\n",
            "Step: 85, Action: 2, Next State: [-0.5657478   0.00364551], Reward: -1.0, Done: False\n",
            "Step: 86, Action: 2, Next State: [-0.560787    0.00496078], Reward: -1.0, Done: False\n",
            "Step: 87, Action: 1, Next State: [-0.55554795  0.00523912], Reward: -1.0, Done: False\n",
            "Step: 88, Action: 0, Next State: [-0.55106956  0.00447837], Reward: -1.0, Done: False\n",
            "Step: 89, Action: 0, Next State: [-0.5473854   0.00368416], Reward: -1.0, Done: False\n",
            "Step: 90, Action: 1, Next State: [-0.54352295  0.00386241], Reward: -1.0, Done: False\n",
            "Step: 91, Action: 0, Next State: [-0.54051125  0.00301176], Reward: -1.0, Done: False\n",
            "Step: 92, Action: 2, Next State: [-0.53637266  0.00413854], Reward: -1.0, Done: False\n",
            "Step: 93, Action: 0, Next State: [-0.53313833  0.00323433], Reward: -1.0, Done: False\n",
            "Step: 94, Action: 1, Next State: [-0.5298325   0.00330586], Reward: -1.0, Done: False\n",
            "Step: 95, Action: 0, Next State: [-0.5274799   0.00235261], Reward: -1.0, Done: False\n",
            "Step: 96, Action: 0, Next State: [-0.52609813  0.00138172], Reward: -1.0, Done: False\n",
            "Step: 97, Action: 2, Next State: [-0.5236977   0.00240047], Reward: -1.0, Done: False\n",
            "Step: 98, Action: 0, Next State: [-0.5222965   0.00140121], Reward: -1.0, Done: False\n",
            "Step: 99, Action: 1, Next State: [-0.5209051   0.00139144], Reward: -1.0, Done: False\n",
            "Step: 100, Action: 1, Next State: [-0.5195338   0.00137124], Reward: -1.0, Done: False\n",
            "Step: 101, Action: 2, Next State: [-0.5171931   0.00234075], Reward: -1.0, Done: False\n",
            "Step: 102, Action: 1, Next State: [-0.5149003   0.00229271], Reward: -1.0, Done: False\n",
            "Step: 103, Action: 0, Next State: [-0.5136729   0.00122748], Reward: -1.0, Done: False\n",
            "Step: 104, Action: 0, Next State: [-5.1351982e-01  1.5304703e-04], Reward: -1.0, Done: False\n",
            "Step: 105, Action: 1, Next State: [-5.1344234e-01  7.7466386e-05], Reward: -1.0, Done: False\n",
            "Step: 106, Action: 0, Next State: [-0.5144411 -0.0009987], Reward: -1.0, Done: False\n",
            "Step: 107, Action: 0, Next State: [-0.5165084  -0.00206737], Reward: -1.0, Done: False\n",
            "Step: 108, Action: 1, Next State: [-0.51862895 -0.00212054], Reward: -1.0, Done: False\n",
            "Step: 109, Action: 2, Next State: [-0.5197868  -0.00115782], Reward: -1.0, Done: False\n",
            "Step: 110, Action: 1, Next State: [-0.5209732 -0.0011864], Reward: -1.0, Done: False\n",
            "Step: 111, Action: 2, Next State: [-5.2117926e-01 -2.0609630e-04], Reward: -1.0, Done: False\n",
            "Step: 112, Action: 0, Next State: [-0.52240354 -0.00122424], Reward: -1.0, Done: False\n",
            "Step: 113, Action: 1, Next State: [-0.5236367  -0.00123321], Reward: -1.0, Done: False\n",
            "Step: 114, Action: 0, Next State: [-0.52586967 -0.00223292], Reward: -1.0, Done: False\n",
            "Step: 115, Action: 1, Next State: [-0.52808553 -0.00221589], Reward: -1.0, Done: False\n",
            "Step: 116, Action: 2, Next State: [-0.5292678  -0.00118224], Reward: -1.0, Done: False\n",
            "Step: 117, Action: 1, Next State: [-0.5304075  -0.00113973], Reward: -1.0, Done: False\n",
            "Step: 118, Action: 1, Next State: [-0.53149617 -0.00108866], Reward: -1.0, Done: False\n",
            "Step: 119, Action: 0, Next State: [-0.5335256  -0.00202944], Reward: -1.0, Done: False\n",
            "Step: 120, Action: 2, Next State: [-0.53448063 -0.000955  ], Reward: -1.0, Done: False\n",
            "Step: 121, Action: 2, Next State: [-5.3435403e-01  1.2660080e-04], Reward: -1.0, Done: False\n",
            "Step: 122, Action: 0, Next State: [-0.5351468  -0.00079275], Reward: -1.0, Done: False\n",
            "Step: 123, Action: 1, Next State: [-0.5358529  -0.00070616], Reward: -1.0, Done: False\n",
            "Step: 124, Action: 2, Next State: [-5.3546721e-01  3.8572887e-04], Reward: -1.0, Done: False\n",
            "Step: 125, Action: 2, Next State: [-0.53399247  0.00147472], Reward: -1.0, Done: False\n",
            "Step: 126, Action: 2, Next State: [-0.5314398   0.00255266], Reward: -1.0, Done: False\n",
            "Step: 127, Action: 0, Next State: [-0.5298283   0.00161147], Reward: -1.0, Done: False\n",
            "Step: 128, Action: 2, Next State: [-0.5271701   0.00265818], Reward: -1.0, Done: False\n",
            "Step: 129, Action: 0, Next State: [-0.52548516  0.00168497], Reward: -1.0, Done: False\n",
            "Step: 130, Action: 0, Next State: [-0.52478606  0.00069912], Reward: -1.0, Done: False\n",
            "Step: 131, Action: 1, Next State: [-0.524078    0.00070802], Reward: -1.0, Done: False\n",
            "Step: 132, Action: 1, Next State: [-0.52336645  0.00071162], Reward: -1.0, Done: False\n",
            "Step: 133, Action: 0, Next State: [-5.2365655e-01 -2.9012654e-04], Reward: -1.0, Done: False\n",
            "Step: 134, Action: 0, Next State: [-0.5249463  -0.00128969], Reward: -1.0, Done: False\n",
            "Step: 135, Action: 1, Next State: [-0.5262258  -0.00127959], Reward: -1.0, Done: False\n",
            "Step: 136, Action: 0, Next State: [-0.5284857  -0.00225988], Reward: -1.0, Done: False\n",
            "Step: 137, Action: 1, Next State: [-0.53070897 -0.00222323], Reward: -1.0, Done: False\n",
            "Step: 138, Action: 2, Next State: [-0.5318789  -0.00116991], Reward: -1.0, Done: False\n",
            "Step: 139, Action: 2, Next State: [-5.3198665e-01 -1.0781732e-04], Reward: -1.0, Done: False\n",
            "Step: 140, Action: 0, Next State: [-0.5330316  -0.00104491], Reward: -1.0, Done: False\n",
            "Step: 141, Action: 2, Next State: [-5.3300577e-01  2.5821979e-05], Reward: -1.0, Done: False\n",
            "Step: 142, Action: 0, Next State: [-0.5339094  -0.00090363], Reward: -1.0, Done: False\n",
            "Step: 143, Action: 0, Next State: [-0.5357357  -0.00182632], Reward: -1.0, Done: False\n",
            "Step: 144, Action: 0, Next State: [-0.53847104 -0.00273531], Reward: -1.0, Done: False\n",
            "Step: 145, Action: 1, Next State: [-0.54109484 -0.00262381], Reward: -1.0, Done: False\n",
            "Step: 146, Action: 2, Next State: [-0.54258746 -0.00149265], Reward: -1.0, Done: False\n",
            "Step: 147, Action: 2, Next State: [-5.429378e-01 -3.503071e-04], Reward: -1.0, Done: False\n",
            "Step: 148, Action: 0, Next State: [-0.54414314 -0.00120535], Reward: -1.0, Done: False\n",
            "Step: 149, Action: 2, Next State: [-5.4419452e-01 -5.1360654e-05], Reward: -1.0, Done: False\n",
            "Step: 150, Action: 2, Next State: [-0.5430915   0.00110301], Reward: -1.0, Done: False\n",
            "Step: 151, Action: 1, Next State: [-0.54184234  0.00124912], Reward: -1.0, Done: False\n",
            "Step: 152, Action: 0, Next State: [-5.4145646e-01  3.8587968e-04], Reward: -1.0, Done: False\n",
            "Step: 153, Action: 2, Next State: [-0.5399367   0.00151975], Reward: -1.0, Done: False\n",
            "Step: 154, Action: 1, Next State: [-0.5382945   0.00164223], Reward: -1.0, Done: False\n",
            "Step: 155, Action: 1, Next State: [-0.5365421   0.00175242], Reward: -1.0, Done: False\n",
            "Step: 156, Action: 2, Next State: [-0.5336926   0.00284947], Reward: -1.0, Done: False\n",
            "Step: 157, Action: 2, Next State: [-0.52976745  0.00392516], Reward: -1.0, Done: False\n",
            "Step: 158, Action: 0, Next State: [-0.52679604  0.00297142], Reward: -1.0, Done: False\n",
            "Step: 159, Action: 1, Next State: [-0.5238006  0.0029954], Reward: -1.0, Done: False\n",
            "Step: 160, Action: 0, Next State: [-0.52180374  0.00199691], Reward: -1.0, Done: False\n",
            "Step: 161, Action: 1, Next State: [-0.5198203   0.00198345], Reward: -1.0, Done: False\n",
            "Step: 162, Action: 2, Next State: [-0.51686513  0.00295511], Reward: -1.0, Done: False\n",
            "Step: 163, Action: 0, Next State: [-0.5149605   0.00190462], Reward: -1.0, Done: False\n",
            "Step: 164, Action: 2, Next State: [-0.5121207   0.00283984], Reward: -1.0, Done: False\n",
            "Step: 165, Action: 1, Next State: [-0.5093669   0.00275377], Reward: -1.0, Done: False\n",
            "Step: 166, Action: 2, Next State: [-0.5057199   0.00364706], Reward: -1.0, Done: False\n",
            "Step: 167, Action: 2, Next State: [-0.5012069   0.00451303], Reward: -1.0, Done: False\n",
            "Step: 168, Action: 0, Next State: [-0.49786162  0.00334522], Reward: -1.0, Done: False\n",
            "Step: 169, Action: 2, Next State: [-0.49370924  0.00415238], Reward: -1.0, Done: False\n",
            "Step: 170, Action: 0, Next State: [-0.49078074  0.00292851], Reward: -1.0, Done: False\n",
            "Step: 171, Action: 2, Next State: [-0.48709795  0.00368277], Reward: -1.0, Done: False\n",
            "Step: 172, Action: 2, Next State: [-0.4826884   0.00440957], Reward: -1.0, Done: False\n",
            "Step: 173, Action: 1, Next State: [-0.4785849   0.00410351], Reward: -1.0, Done: False\n",
            "Step: 174, Action: 0, Next State: [-0.47581795  0.00276693], Reward: -1.0, Done: False\n",
            "Step: 175, Action: 2, Next State: [-0.47240818  0.0034098 ], Reward: -1.0, Done: False\n",
            "Step: 176, Action: 1, Next State: [-0.4693808   0.00302738], Reward: -1.0, Done: False\n",
            "Step: 177, Action: 2, Next State: [-0.46575826  0.00362253], Reward: -1.0, Done: False\n",
            "Step: 178, Action: 1, Next State: [-0.46256736  0.0031909 ], Reward: -1.0, Done: False\n",
            "Step: 179, Action: 1, Next State: [-0.45983163  0.00273572], Reward: -1.0, Done: False\n",
            "Step: 180, Action: 1, Next State: [-0.45757127  0.00226038], Reward: -1.0, Done: False\n",
            "Step: 181, Action: 2, Next State: [-0.45480287  0.0027684 ], Reward: -1.0, Done: False\n",
            "Step: 182, Action: 0, Next State: [-0.45354676  0.00125609], Reward: -1.0, Done: False\n",
            "Step: 183, Action: 0, Next State: [-4.5381221e-01 -2.6544326e-04], Reward: -1.0, Done: False\n",
            "Step: 184, Action: 0, Next State: [-0.45559725 -0.00178503], Reward: -1.0, Done: False\n",
            "Step: 185, Action: 2, Next State: [-0.45688874 -0.00129151], Reward: -1.0, Done: False\n",
            "Step: 186, Action: 1, Next State: [-0.45867726 -0.0017885 ], Reward: -1.0, Done: False\n",
            "Step: 187, Action: 1, Next State: [-0.4609496  -0.00227234], Reward: -1.0, Done: False\n",
            "Step: 188, Action: 2, Next State: [-0.46268904 -0.00173945], Reward: -1.0, Done: False\n",
            "Step: 189, Action: 1, Next State: [-0.46488276 -0.00219373], Reward: -1.0, Done: False\n",
            "Step: 190, Action: 2, Next State: [-0.4665146  -0.00163183], Reward: -1.0, Done: False\n",
            "Step: 191, Action: 2, Next State: [-0.46757248 -0.00105787], Reward: -1.0, Done: False\n",
            "Step: 192, Action: 1, Next State: [-0.46904856 -0.00147609], Reward: -1.0, Done: False\n",
            "Step: 193, Action: 1, Next State: [-0.47093195 -0.00188339], Reward: -1.0, Done: False\n",
            "Step: 194, Action: 1, Next State: [-0.4732087  -0.00227675], Reward: -1.0, Done: False\n",
            "Step: 195, Action: 2, Next State: [-0.47486195 -0.00165324], Reward: -1.0, Done: False\n",
            "Step: 196, Action: 0, Next State: [-0.4778794  -0.00301747], Reward: -1.0, Done: False\n",
            "Step: 197, Action: 1, Next State: [-0.4812387  -0.00335929], Reward: -1.0, Done: False\n",
            "Step: 198, Action: 2, Next State: [-0.48391485 -0.00267613], Reward: -1.0, Done: False\n",
            "Step: 199, Action: 0, Next State: [-0.4878879  -0.00397306], Reward: -1.0, Done: True\n",
            "Episode finished after 200 steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "id": "ODm1klayH_II"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNIVJ3haLAjV",
        "outputId": "6c4378af-8be5-4881-9486-a128d0df6079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ale-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOheesIuL098",
        "outputId": "3da18ab3-befc-46f4-d7ad-151a70de61bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.7.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ale-py) (1.23.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main():\n",
        "    # Set the environment name and version\n",
        "    env_name = 'Assault'\n",
        "    env_version = 'v4'\n",
        "\n",
        "    # Create the environment\n",
        "    env_id = f'{env_name}-{env_version}'\n",
        "    env = gym.make(env_id)\n",
        "\n",
        "    # Reset the environment to get the initial observation\n",
        "    observation = env.reset()\n",
        "\n",
        "    # Set up Matplotlib figure\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Display the initial observation\n",
        "    img = ax.imshow(observation, interpolation='none')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Main loop\n",
        "    done = False\n",
        "    while not done:\n",
        "        # Take a random action for simplicity (replace with your RL agent)\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        # Step in the environment\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the displayed image\n",
        "        img.set_array(observation)\n",
        "\n",
        "        # Pause for a short time to visualize the environment\n",
        "        plt.pause(0.01)\n",
        "\n",
        "    # Close the environment when done\n",
        "    env.close()\n",
        "\n",
        "    # Show the final visualization\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "0G-cv9AkK_Nl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}